{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ Unsloth å¯¹ DeepSeek-R1-Distill-Qwen-1.5B æ¨¡å‹è¿›è¡Œ LoRA å¾®è°ƒ\n",
    "\n",
    "æœ¬ Notebook å±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ `unsloth` åº“å¯¹ `deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B` æ¨¡å‹è¿›è¡Œé«˜æ•ˆçš„ QLoRA (Low-Rank Adaptation) å¾®è°ƒã€‚\n",
    "\n",
    "æ•´ä¸ªæµç¨‹åŒ…æ‹¬ï¼š\n",
    "1.  ç¯å¢ƒå‡†å¤‡ä¸åº“å¯¼å…¥\n",
    "2.  åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ (Tokenizer)ã€‚\n",
    "3.  åœ¨å¾®è°ƒå‰ï¼Œå¯¹æ¨¡å‹è¿›è¡Œç®€å•çš„æ¨ç†æµ‹è¯•ã€‚\n",
    "4.  ä¸‹è½½å’Œæ ¼å¼åŒ–è®­ç»ƒæ•°æ®é›†\n",
    "5.  ä½¿ç”¨ `unsloth` çš„ `FastLanguageModel` æ¥ä¸ºæ¨¡å‹æ·»åŠ  LoRA é€‚é…å™¨ã€‚\n",
    "6.  é…ç½® `SFTTrainer` ç›‘ç£å¾®è°ƒè®­ç»ƒé…ç½®ã€‚\n",
    "7.  å¯åŠ¨è®­ç»ƒï¼Œå¹¶è§‚å¯Ÿ Loss å˜åŒ–æƒ…å†µ\n",
    "8.  ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹\n",
    "9.  æµ‹è¯•è®­ç»ƒåçš„ç”Ÿæˆç»“æœ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ç¯å¢ƒå‡†å¤‡ä¸åº“å¯¼å…¥\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å®‰è£…å¹¶å¯¼å…¥æ‰€æœ‰å¿…è¦çš„åº“ã€‚`transformers` ç”¨äºåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œ`unsloth` ç”¨äºé«˜æ•ˆå¾®è°ƒï¼Œ`trl` æä¾›äº† `SFTTrainer`ï¼Œè€Œ `datasets` ç”¨äºå¤„ç†æ•°æ®ã€‚\n",
    "\n",
    "**æ³¨æ„**: åœ¨è¿è¡Œæ­¤ Notebook ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰ä¾èµ–åŒ…ï¼š\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig, DataCollatorForSeq2Seq\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ (Tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.5: Fast Qwen2 patching. Transformers: 4.55.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.578 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# å®šä¹‰æ¨¡å‹å’Œä¸€äº›åŸºæœ¬å‚æ•°\n",
    "max_seq_length = 8192\n",
    "dtype = None # None è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹© (Float16 a T4, V100, BFloat16 a Ampere)\n",
    "load_in_4bit = True # ä½¿ç”¨ 4bit é‡åŒ–åŠ è½½\n",
    "\n",
    "# è¿™æ˜¯æ‚¨çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œè¯·æ›¿æ¢ä¸ºæ‚¨æ­£åœ¨ä½¿ç”¨çš„æ¨¡å‹\n",
    "# ä¾‹å¦‚ï¼š\"qwen-1.5b_lora_model\"\n",
    "# model_name = \"qwen-1.5b_lora_model\" \n",
    "# model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B\" \n",
    "model_name = \"unsloth/DeepSeek-R1-Distill-Qwen-1.5B-unsloth-bnb-4bit\" \n",
    "\n",
    "# è¿™ä¸€æ­¥ä¼šè¿”å›ä¸€ä¸ªç»è¿‡ Unsloth ä¼˜åŒ–çš„æ¨¡å‹å’Œä¸€ä¸ªåˆ†è¯å™¨\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. å¾®è°ƒå‰æ¨ç†æµ‹è¯•\n",
    "\n",
    "åœ¨å¯¹æ¨¡å‹è¿›è¡Œä»»ä½•ä¿®æ”¹ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆç”¨å®ƒæ¥ç”Ÿæˆä¸€æ®µæ–‡æœ¬ï¼Œçœ‹çœ‹åŸå§‹æ¨¡å‹çš„è¡¨ç°å¦‚ä½•ã€‚è¿™å¯ä»¥ä½œä¸ºæˆ‘ä»¬å¾®è°ƒæ•ˆæœçš„åŸºå‡†å‚è€ƒã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹æ¨ç†çš„ Prompt æ¨¡æ¿\n",
    "inference_prompt = \"\"\"ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
    "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
    "\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "question = \"ç”·ï¼Œ28å²ï¼Œç¨‹åºå‘˜ï¼Œæœ€è¿‘ä¸€å‘¨æ¯å¤©å·¥ä½œåˆ°åŠå¤œï¼Œæ„Ÿè§‰å¤´æ™•ã€è„–å­ç–¼ï¼Œæœ‰æ—¶å€™è¿˜æ¶å¿ƒã€‚\"\n",
    "\n",
    "inputs = tokenizer([inference_prompt.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "attention_mask = inputs.input_ids.ne(tokenizer.pad_token_id).long().to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "å¥½çš„ï¼Œæˆ‘éœ€è¦å¸®åŠ©è¿™ä½28å²çš„ç”·æ€§å¤„ç†è¿‘æœŸçš„å¥åº·é—®é¢˜ã€‚é¦–å…ˆï¼Œä»–æ¯å¤©å·¥ä½œåˆ°åŠå¤œï¼Œå·¥ä½œå¼ºåº¦å¤§ï¼Œå¯èƒ½å¯¹ç¡çœ å’Œç¥ç»ç³»ç»Ÿçš„é€‚åº”æ€§æœ‰å½±å“ã€‚å¤´æ™•ã€è„–å­ç–¼å’Œæ¶å¿ƒï¼Œè¿™äº›ç—‡çŠ¶å¯èƒ½ä¸å‹åŠ›ã€ç–²åŠ³æˆ–ç¥ç»ç³»ç»Ÿçš„ä¸ç¨³å®šæ€§æœ‰å…³ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæˆ‘åº”è¯¥è€ƒè™‘æ˜¯å¦æ˜¯å¸¸è§çš„è„‘è¡€ç®¡ç–¾ç—…ï¼Œå¦‚ä¸­é£æˆ–è„‘æ¢—æ­»ã€‚å¦‚æœå‡ºç°æ™•å¥æˆ–æ¶å¿ƒï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥è¯„ä¼°æ˜¯å¦æœ‰è¯¯è¯Šã€‚åŒæ—¶ï¼Œå·¥ä½œå‹åŠ›å¯èƒ½å¯¹ç¥ç»ç³»ç»Ÿäº§ç”Ÿå½±å“ï¼Œå»ºè®® himä¿æŒè‰¯å¥½çš„ä¼‘æ¯ä¹ æƒ¯ï¼Œå‡å°‘å·¥ä½œå‹åŠ›ï¼Œæ¯”å¦‚ä½¿ç”¨ä¼‘æ¯æ—¥æˆ–å†¥æƒ³æ¥ç¼“è§£å‹åŠ›ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œè€ƒè™‘æ˜¯å¦æœ‰å…¶ä»–æ½œåœ¨çš„å¥åº·é—®é¢˜ï¼Œå¦‚é«˜è¡€å‹ã€ç³–å°¿ç—…æˆ–å¿ƒè„ç—…ã€‚å¦‚æœä»–è¿‘æœŸæœ‰å®¶æ—å²ï¼Œå¯èƒ½éœ€è¦è¿›è¡Œè¯¦ç»†æ£€æŸ¥ï¼ŒåŒ…æ‹¬å¿ƒç”µå›¾å’Œè¡€å‹ç›‘æµ‹ã€‚å¦‚æœç—‡çŠ¶æŒç»­æˆ–åŠ é‡ï¼Œå»ºè®®ä»–å¯»æ±‚ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—ï¼Œä»¥ç¡®ä¿é•¿æœŸå¥åº·ã€‚\n",
      "\n",
      "æœ€åï¼Œæé†’ä»–å…³æ³¨é¥®é£Ÿå’Œè¿åŠ¨ï¼Œä»¥æ”¹å–„èº«ä½“çŠ¶æ€ã€‚å¦‚æœç—‡çŠ¶ä¸¥é‡ï¼ŒåŠæ—¶å°±åŒ»æ˜¯å…³é”®ã€‚\n",
      "</think>\n",
      "\n",
      "ç”·ï¼Œ28å²ï¼Œç¨‹åºå‘˜ï¼Œæœ€è¿‘ä¸€å‘¨æ¯å¤©å·¥ä½œåˆ°åŠå¤œï¼Œæ„Ÿè§‰å¤´æ™•ã€è„–å­ç–¼ï¼Œæœ‰æ—¶å€™è¿˜æ¶å¿ƒã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œè€ƒè™‘æ˜¯å¦æ˜¯å¸¸è§çš„è„‘è¡€ç®¡ç–¾ç—…ï¼Œå¦‚ä¸­é£æˆ–è„‘æ¢—æ­»ã€‚å¤´æ™•ã€æ¶å¿ƒå¯èƒ½ä¸å‹åŠ›ã€ç–²åŠ³æˆ–ç¥ç»ç³»ç»Ÿçš„ä¸ç¨³å®šæ€§æœ‰å…³ã€‚å»ºè®®ä»–ä¿æŒè‰¯å¥½çš„ä¼‘æ¯ä¹ æƒ¯ï¼Œå‡å°‘å·¥ä½œå‹åŠ›ï¼Œæ¯”å¦‚ä½¿ç”¨ä¼‘æ¯æ—¥æˆ–å†¥æƒ³æ¥ç¼“è§£å‹åŠ›ã€‚\n",
      "\n",
      "å…¶æ¬¡ï¼Œè€ƒè™‘æ˜¯å¦æœ‰é«˜è¡€å‹ã€ç³–å°¿ç—…æˆ–å¿ƒè„ç—…çš„å¯èƒ½ã€‚å¦‚æœè¿‘æœŸæœ‰å®¶æ—å²ï¼Œå»ºè®®è¿›è¡Œè¯¦ç»†æ£€æŸ¥ï¼ŒåŒ…æ‹¬å¿ƒç”µå›¾å’Œè¡€å‹ç›‘æµ‹ã€‚\n",
      "\n",
      "å¦‚æœç—‡çŠ¶æŒç»­æˆ–åŠ é‡ï¼Œå»ºè®®å°½å¿«å¯»æ±‚ä¸“ä¸šåŒ»ç”Ÿçš„è¯Šæ–­å’Œæ²»ç–—ï¼Œä»¥ç¡®ä¿é•¿æœŸå¥åº·ã€‚\n",
      "\n",
      "æœ€åï¼Œæé†’ä»–å…³æ³¨é¥®é£Ÿå’Œè¿åŠ¨ï¼Œä»¥æ”¹å–„èº«ä½“çŠ¶æ€ã€‚å¦‚æœç—‡çŠ¶ä¸¥é‡ï¼ŒåŠæ—¶å°±åŒ»æ˜¯å…³é”®ã€‚\n"
     ]
    }
   ],
   "source": [
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. ä¸‹è½½å’Œæ ¼å¼åŒ–è®­ç»ƒæ•°æ®é›†\n",
    "\n",
    "\n",
    "åŒ»å­¦æ¨ç†æ•°æ®é›†ï¼šhttps://huggingface.co/datasets/FreedomIntelligence/medical-o1-reasoning-SFT/viewer/zh\n",
    "\n",
    "![dataset](images/dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹è®­ç»ƒçš„ Prompt æ¨¡æ¿\n",
    "train_prompt = \"\"\"ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
    "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
    "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
    "\n",
    "\n",
    "### Instruction:\n",
    "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
    "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token # æ·»åŠ  EOS Token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        # å°† EOS Token æ·»åŠ åˆ°æ ·æœ¬æœ€å\n",
    "        text = train_prompt.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "pass\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"zh\", split = \"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\\nè¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\\nåœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\\n\\n\\n### Instruction:\\nä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\\nè¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\\n\\n### Question:\\næ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ\\n\\n### Response:\\n<think>\\nè¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\\n\\nç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\\n\\nä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\\n\\nç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\\n\\nä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\\n\\nå›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\\n\\nå†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\\n\\nçœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\\n\\nä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\\n\\nå“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚\\n</think>\\nä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\\n<ï½œendâ–ofâ–sentenceï½œ>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ä»¥ä¸‹æ˜¯ä¸€æ¡æè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œå¹¶é…æœ‰ä¸€ä¸ªæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚\n",
       "è¯·æ’°å†™ä¸€ä»½æ°å½“çš„å›å¤ï¼Œä»¥å®Œæˆè¯¥è¯·æ±‚ã€‚\n",
       "åœ¨å›ç­”ä¹‹å‰ï¼Œè¯·ä»”ç»†æ€è€ƒè¯¥é—®é¢˜ï¼Œå¹¶æ„å»ºä¸€ä¸ªåˆ†æ­¥çš„æ€è€ƒè¿‡ç¨‹ï¼Œä»¥ç¡®ä¿å›åº”çš„é€»è¾‘ä¸¥è°¨å’Œå†…å®¹å‡†ç¡®ã€‚\n",
       "\n",
       "\n",
       "### Instruction:\n",
       "ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œåœ¨ä¸´åºŠæ¨ç†ã€è¯Šæ–­å­¦å’Œæ²»ç–—è§„åˆ’æ–¹é¢æ‹¥æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†ã€‚\n",
       "è¯·å›ç­”ä»¥ä¸‹åŒ»å­¦é—®é¢˜ã€‚\n",
       "\n",
       "### Question:\n",
       "æ ¹æ®æè¿°ï¼Œä¸€ä¸ª1å²çš„å­©å­åœ¨å¤å­£å¤´çš®å‡ºç°å¤šå¤„å°ç»“èŠ‚ï¼Œé•¿æœŸä¸æ„ˆåˆï¼Œä¸”ç°åœ¨ç–®å¤§å¦‚æ¢…ï¼Œæºƒç ´æµè„“ï¼Œå£ä¸æ”¶æ•›ï¼Œå¤´çš®ä¸‹æœ‰ç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšã€‚è¿™ç§ç—…ç—‡åœ¨ä¸­åŒ»ä¸­è¯Šæ–­ä¸ºä»€ä¹ˆç—…ï¼Ÿ\n",
       "\n",
       "### Response:\n",
       "<think>\n",
       "è¿™ä¸ªå°å­©å­åœ¨å¤å¤©å¤´çš®ä¸Šé•¿äº†äº›å°ç»“èŠ‚ï¼Œä¸€ç›´éƒ½æ²¡å¥½ï¼Œåæ¥å˜æˆäº†è„“åŒ…ï¼Œæµäº†å¥½å¤šè„“ã€‚æƒ³æƒ³å¤å¤©é‚£ä¹ˆçƒ­ï¼Œå¯èƒ½å’Œæ¹¿çƒ­æœ‰å…³ã€‚æ‰ä¸€å²çš„å°å­©ï¼Œå…ç–«åŠ›æœ¬æ¥å°±ä¸å¼ºï¼Œå¤å¤©çš„æ¹¿çƒ­æ²¡å‡†å°±ä¾µè¢­äº†èº«ä½“ã€‚\n",
       "\n",
       "ç”¨ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œå‡ºç°å°ç»“èŠ‚ã€å†åŠ ä¸Šé•¿æœŸä¸æ„ˆåˆï¼Œè¿™äº›ç—‡çŠ¶è®©æˆ‘æƒ³åˆ°äº†å¤´ç–®ã€‚å°å­©å­æœ€å®¹æ˜“å¾—è¿™äº›çš®è‚¤ç—…ï¼Œä¸»è¦å› ä¸ºæ¹¿çƒ­åœ¨ä½“è¡¨éƒç»“ã€‚\n",
       "\n",
       "ä½†å†çœ‹çœ‹ï¼Œå¤´çš®ä¸‹è¿˜æœ‰ç©ºæ´ï¼Œè¿™å¯èƒ½ä¸æ­¢æ˜¯ç®€å•çš„å¤´ç–®ã€‚çœ‹èµ·æ¥ç—…æƒ…æŒºä¸¥é‡çš„ï¼Œä¹Ÿè®¸æ˜¯è„“è‚¿æ²¡æ²»å¥½ã€‚è¿™æ ·çš„æƒ…å†µä¸­åŒ»ä¸­æœ‰æ—¶å€™å«åšç¦¿ç–®æˆ–è€…æ¹¿ç–®ï¼Œä¹Ÿå¯èƒ½æ˜¯å¦ä¸€ç§æƒ…å†µã€‚\n",
       "\n",
       "ç­‰ä¸€ä¸‹ï¼Œå¤´çš®ä¸Šçš„ç©ºæ´å’Œçš®è‚¤å¢åšæ›´åƒæ˜¯ç–¾ç—…å·²ç»æ·±å…¥åˆ°å¤´çš®ä¸‹ï¼Œè¿™æ˜¯ä¸æ˜¯è¯´æ˜æœ‰å¯èƒ½æ˜¯æµæ³¨æˆ–ç˜°ç–¬ï¼Ÿè¿™äº›åå­—å¸¸æè¿°å¤´éƒ¨æˆ–é¢ˆéƒ¨çš„ä¸¥é‡æ„ŸæŸ“ï¼Œç‰¹åˆ«æ˜¯æœ‰åŒ–è„“ä¸æ„ˆåˆï¼Œåˆå½¢æˆé€šé“æˆ–ç©ºæ´çš„æƒ…å†µã€‚\n",
       "\n",
       "ä»”ç»†æƒ³æƒ³ï¼Œæˆ‘æ€ä¹ˆæ„Ÿè§‰è¿™äº›ç—‡çŠ¶æ›´è´´è¿‘ç˜°ç–¬çš„è¡¨ç°ï¼Ÿå°¤å…¶è€ƒè™‘åˆ°å­©å­çš„å¹´çºªå’Œå¤å¤©å‘ç”Ÿçš„å­£èŠ‚æ€§å› ç´ ï¼Œæ¹¿çƒ­å¯èƒ½æ˜¯ä¸»å› ï¼Œä½†å¯èƒ½ä¹Ÿæœ‰ç«æ¯’æˆ–è€…ç—°æ¹¿é€ æˆçš„æ»ç•™ã€‚\n",
       "\n",
       "å›åˆ°åŸºæœ¬çš„ç—‡çŠ¶æè¿°ä¸Šçœ‹ï¼Œè¿™ç§é•¿æœŸä¸æ„ˆåˆåˆå¤æ‚çš„çŠ¶å†µï¼Œå¦‚æœç»“åˆä¸­åŒ»æ›´åé‡çš„ç—…åï¼Œæ˜¯ä¸æ˜¯æœ‰å¯èƒ½æ˜¯æ¶‰åŠæ›´æ·±å±‚æ¬¡çš„æ„ŸæŸ“ï¼Ÿ\n",
       "\n",
       "å†è€ƒè™‘ä¸€ä¸‹ï¼Œè¿™åº”è¯¥ä¸æ˜¯å•çº¯çš„ç˜°ç–¬ï¼Œå¾—ä»”ç»†åˆ†æå¤´çš®å¢åšå¹¶å‡ºç°ç©ºæ´è¿™æ ·çš„ä¸¥é‡ç—‡çŠ¶ã€‚ä¸­åŒ»é‡Œå¤´ï¼Œè¿™æ ·çš„è¡¨ç°å¯èƒ½æ›´ç¬¦åˆâ€˜èš€ç–®â€™æˆ–â€˜å¤´ç–½â€™ã€‚è¿™äº›ç—…åé€šå¸¸æè¿°å¤´éƒ¨ä¸¥é‡æ„ŸæŸ“åçš„æºƒçƒ‚å’Œç»„ç»‡åæ­»ã€‚\n",
       "\n",
       "çœ‹çœ‹å­£èŠ‚å’Œå­©å­çš„ä½“è´¨ï¼Œå¤å¤©åˆæ¹¿åˆçƒ­ï¼Œå¤–é‚ªå¾ˆå®¹æ˜“ä¾µå…¥å¤´éƒ¨ï¼Œå¯¹å­©å­è¿™ä¹ˆå¼±çš„å…ç–«ç³»ç»Ÿç®€ç›´å°±æ˜¯æŒ‘æˆ˜ã€‚å¤´ç–½è¿™ä¸ªç—…åå¬èµ·æ¥çœŸæ˜¯åˆ‡åˆï¼Œå› ä¸ºå®ƒæè¿°çš„æ„ŸæŸ“ä¸¥é‡ï¼Œæºƒçƒ‚åˆ°å‡ºç°ç©ºæ´ã€‚\n",
       "\n",
       "ä¸è¿‡ï¼Œä»”ç»†ç¢ç£¨åå‘ç°ï¼Œè¿˜æœ‰ä¸ªç—…åä¼¼ä¹æ›´ä¸ºåˆé€‚ï¼Œå«åšâ€˜è¼è›„ç––â€™ï¼Œè¿™ç—…åœ¨ä¸­åŒ»é‡Œä¸“æŒ‡åƒè¿™ç§ä¸¥é‡æ„ŸæŸ“å¹¶ä¼´æœ‰æ·±éƒ¨ç©ºæ´çš„æƒ…å†µã€‚å®ƒä¹Ÿæ¶µç›–äº†åŒ–è„“å’Œçš®è‚¤å¢åšè¿™äº›ç—‡çŠ¶ã€‚\n",
       "\n",
       "å“¦ï¼Œè¯¥ä¸ä¼šæ˜¯å¤å­£æ¹¿çƒ­ï¼Œå¯¼è‡´æ¹¿æ¯’å…¥ä¾µï¼Œå­©å­çš„ä½“è´¨ä¸èƒ½å¾¡ï¼Œå…¶ç—…æƒ…å‘å±•æˆè¿™æ ·çš„æ„ŸæŸ“ï¼Ÿç»¼åˆåˆ†æåæˆ‘è§‰å¾—â€˜è¼è›„ç––â€™è¿™ä¸ªç—…åçœŸæ˜¯ç›¸å½“ç¬¦åˆã€‚\n",
       "</think>\n",
       "ä»ä¸­åŒ»çš„è§’åº¦æ¥çœ‹ï¼Œä½ æ‰€æè¿°çš„ç—‡çŠ¶ç¬¦åˆâ€œè¼è›„ç––â€çš„ç—…ç—‡ã€‚è¿™ç§ç—…ç—‡é€šå¸¸å‘ç”Ÿåœ¨å¤´çš®ï¼Œè¡¨ç°ä¸ºå¤šå¤„ç»“èŠ‚ï¼Œæºƒç ´æµè„“ï¼Œå½¢æˆç©ºæ´ï¼Œæ‚£å¤„çš®è‚¤å¢åšä¸”é•¿æœŸä¸æ„ˆåˆã€‚æ¹¿çƒ­è¾ƒé‡çš„å¤å­£æ›´å®¹æ˜“å¯¼è‡´è¿™ç§ç—…ç—‡çš„å‘å±•ï¼Œç‰¹åˆ«æ˜¯åœ¨å…ç–«åŠ›è¾ƒå¼±çš„å„¿ç«¥èº«ä¸Šã€‚å»ºè®®ç»“åˆä¸­åŒ»çš„æ¸…çƒ­è§£æ¯’ã€ç¥›æ¹¿æ¶ˆè‚¿çš„æ²»ç–—æ–¹æ³•è¿›è¡Œå¤„ç†ï¼Œå¹¶é…åˆä¸“ä¸šçš„åŒ»ç–—å»ºè®®è¿›è¡Œè¯¦ç»†è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
       "<ï½œendâ–ofâ–sentenceï½œ>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(dataset[0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. ä½¿ç”¨ Unsloth æ·»åŠ  LoRA é€‚é…å™¨\n",
    "\n",
    "è¿™æ˜¯ä½¿ç”¨ `unsloth` çš„æ ¸å¿ƒæ­¥éª¤ã€‚æˆ‘ä»¬è°ƒç”¨ `FastLanguageModel.get_peft_model`ï¼Œå®ƒä¼šéå¸¸é«˜æ•ˆåœ°ä¸ºæ¨¡å‹æ³¨å…¥ LoRA æ¨¡å—ã€‚\n",
    "\n",
    "- `r`: LoRA çš„ç§© (rank)ï¼Œæ˜¯æ§åˆ¶æ¨¡å‹å¤æ‚åº¦å’Œå‚æ•°é‡çš„å…³é”®è¶…å‚æ•°ã€‚\n",
    "- `target_modules`: æŒ‡å®šè¦åœ¨å“ªäº›çº¿æ€§å±‚ï¼ˆå¦‚æ³¨æ„åŠ›æœºåˆ¶çš„ q, k, v, o æŠ•å½±å±‚ï¼‰ä¸Šåº”ç”¨ LoRAã€‚\n",
    "- `lora_alpha`: LoRA çš„ç¼©æ”¾å› å­ï¼Œé€šå¸¸è®¾ç½®ä¸º `r` çš„ä¸¤å€æˆ–ä¸ `r` ç›¸åŒã€‚\n",
    "- `use_gradient_checkpointing`: ä¸€ç§èŠ‚çœæ˜¾å­˜çš„æŠ€æœ¯ï¼Œå¯¹äºè®­ç»ƒå¤§æ¨¡å‹è‡³å…³é‡è¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.8.5 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen2ForCausalLM(\n",
      "      (model): Qwen2Model(\n",
      "        (embed_tokens): Embedding(151936, 1536, padding_idx=151654)\n",
      "        (layers): ModuleList(\n",
      "          (0): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "          (1-2): 2 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "          (3-25): 23 x Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "          (26): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "          (27): Qwen2DecoderLayer(\n",
      "            (self_attn): Qwen2Attention(\n",
      "              (q_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): Qwen2MLP(\n",
      "              (gate_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=1536, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=8960, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Identity()\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=8960, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=1536, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# å› ä¸º `model` å¯¹è±¡ç°åœ¨æ˜¯ç”± Unsloth åˆ›å»ºçš„ï¼Œå®ƒåŒ…å«äº†æ‰€æœ‰å¿…éœ€çš„å±æ€§\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\n",
    "      \"q_proj\",\n",
    "      \"k_proj\",\n",
    "      \"v_proj\",\n",
    "      \"o_proj\",\n",
    "      \"gate_proj\",\n",
    "      \"up_proj\",\n",
    "      \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=1432,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "# æ£€æŸ¥æ¨¡å‹ç»“æ„ï¼Œç¡®è®¤ LoRA é€‚é…å™¨å·²æ·»åŠ \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. é…ç½® SFTTrainer\n",
    "\n",
    "`SFTTrainer` (Supervised Fine-tuning Trainer) æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºæŒ‡ä»¤å¾®è°ƒçš„è®­ç»ƒå™¨ã€‚æˆ‘ä»¬éœ€è¦é…ç½® `TrainingArguments` æ¥æŒ‡å®šæ‰€æœ‰çš„è®­ç»ƒå‚æ•°ï¼Œå¦‚æ‰¹é‡å¤§å°ã€å­¦ä¹ ç‡ã€ä¼˜åŒ–å™¨ç­‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig, SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 64,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 5,\n",
    "        # num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        max_steps = 60,\n",
    "        learning_rate = 2e-4,\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 1432,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. å¼€å§‹è®­ç»ƒ\n",
    "\n",
    "ä¸€åˆ‡å‡†å¤‡å°±ç»ªåï¼Œè°ƒç”¨ `trainer.train()` å³å¯å¼€å§‹å¾®è°ƒè¿‡ç¨‹ã€‚è®­ç»ƒç»“æŸåï¼Œä¼šè¿”å›åŒ…å«è®­ç»ƒç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚è®­ç»ƒæŸå¤±ï¼‰çš„å¯¹è±¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 20,171 | Num Epochs = 1 | Total steps = 60\n",
      "O^O/ \\_/ \\    Batch size per device = 64 | Gradient accumulation steps = 2\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (64 x 2 x 1) = 128\n",
      " \"-____-\"     Trainable parameters = 18,464,768 of 1,795,552,768 (1.03% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA driver error: invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m trainer_stats = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(trainer_stats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/transformers/trainer.py:2238\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2236\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2237\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2239\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/accelerate/utils/memory.py:174\u001b[39m, in \u001b[36mfind_executable_batch_size.<locals>.decorator\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo executable batch size found, reached zero.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    173\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m should_reduce_batch_size(e):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:325\u001b[39m, in \u001b[36m_fast_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deepseek-quickstart/distill/unsloth_compiled_cache/UnslothSFTTrainer.py:999\u001b[39m, in \u001b[36m_UnslothSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    997\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    998\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m--> \u001b[39m\u001b[32m999\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:34\u001b[39m, in \u001b[36m_unsloth_training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/deepseek-quickstart/distill/unsloth_compiled_cache/UnslothSFTTrainer.py:988\u001b[39m, in \u001b[36m_UnslothSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, model, inputs, return_outputs = \u001b[38;5;28;01mFalse\u001b[39;00m, num_items_in_batch = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m988\u001b[39m     outputs = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    994\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/unsloth/models/_utils.py:1197\u001b[39m, in \u001b[36m_unsloth_pre_compute_loss\u001b[39m\u001b[34m(self, model, inputs, *args, **kwargs)\u001b[39m\n\u001b[32m   1191\u001b[39m     logger.warning_once(\n\u001b[32m   1192\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Not an error, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept `num_items_in_batch`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1193\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing gradient accumulation will be very slightly less accurate.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\\\n\u001b[32m   1194\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRead more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1195\u001b[39m     )\n\u001b[32m   1196\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1197\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_old_compute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/transformers/trainer.py:3884\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3882\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3883\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3884\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3885\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3886\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3887\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/accelerate/utils/operations.py:818\u001b[39m, in \u001b[36mconvert_outputs_to_fp32.<locals>.forward\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    817\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/accelerate/utils/operations.py:806\u001b[39m, in \u001b[36mConvertOutputsToFp32.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    805\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/amp/autocast_mode.py:44\u001b[39m, in \u001b[36mautocast_decorator.<locals>.decorate_autocast\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_autocast\u001b[39m(*args, **kwargs):\n\u001b[32m     43\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/_compile.py:51\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/unsloth/models/llama.py:1331\u001b[39m, in \u001b[36mPeftModel_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, num_logits_to_keep, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_model(\n\u001b[32m   1321\u001b[39m         input_ids = input_ids,\n\u001b[32m   1322\u001b[39m         attention_mask = attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1328\u001b[39m         **kwargs,\n\u001b[32m   1329\u001b[39m     )\n\u001b[32m   1330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1331\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1332\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1335\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1337\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1338\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1339\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1340\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_logits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1341\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1342\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1343\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/peft/tuners/tuners_utils.py:222\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/unsloth/models/llama.py:1142\u001b[39m, in \u001b[36mCausalLM_fast_forward.<locals>._CausalLM_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, num_logits_to_keep, logits_to_keep, *args, **kwargs)\u001b[39m\n\u001b[32m   1140\u001b[39m     \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m   1141\u001b[39m     \u001b[38;5;28mself\u001b[39m.model._has_no_labels = labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1142\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1143\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1144\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1155\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/unsloth/models/llama.py:954\u001b[39m, in \u001b[36mLlamaModel_fast_forward\u001b[39m\u001b[34m(self, input_ids, causal_mask, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, *args, **kwargs)\u001b[39m\n\u001b[32m    951\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    953\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m954\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    965\u001b[39m     hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/transformers/modeling_layers.py:93\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m         message = message.rstrip(\u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m) + \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning(message)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/_compile.py:51\u001b[39m, in \u001b[36m_disable_dynamo.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m     disable_fn = torch._dynamo.disable(fn, recursive)\n\u001b[32m     49\u001b[39m     fn.__dynamo_disable = disable_fn  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py:838\u001b[39m, in \u001b[36mDisableContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    836\u001b[39m _maybe_set_eval_frame(_callback_from_stance(\u001b[38;5;28mself\u001b[39m.callback))\n\u001b[32m    837\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m838\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    840\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/utils/checkpoint.py:488\u001b[39m, in \u001b[36mcheckpoint\u001b[39m\u001b[34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[39m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m    484\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    485\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    486\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muse_reentrant=False.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    487\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    490\u001b[39m     gen = _checkpoint_without_reentrant_generator(\n\u001b[32m    491\u001b[39m         function, preserve, context_fn, determinism_check, debug, *args, **kwargs\n\u001b[32m    492\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/autograd/function.py:575\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    573\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    574\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m575\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    577\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    578\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    579\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/unsloth_zoo/gradient_checkpointing.py:475\u001b[39m, in \u001b[36mUnslothCheckpointFunction.forward\u001b[39m\u001b[34m(ctx, run_function, preserve_rng_state, *args)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ctx._requires_gradient: ctx.save_for_backward(*tensor_inputs)\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     outputs = \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_gpu_buffer: MAIN_STREAM.wait_stream(EXTRA_STREAM)\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/unsloth/models/llama.py:627\u001b[39m, in \u001b[36mLlamaDecoderLayer_fast_forward\u001b[39m\u001b[34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m residual = hidden_states\n\u001b[32m    626\u001b[39m hidden_states = fast_rms_layernorm(\u001b[38;5;28mself\u001b[39m.input_layernorm, hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m hidden_states, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m       \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m         \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m      \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m   \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m           \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m        \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    638\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    640\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/unsloth/models/llama.py:529\u001b[39m, in \u001b[36mLlamaAttention_fast_forward\u001b[39m\u001b[34m(self, hidden_states, causal_mask, attention_mask, position_ids, past_key_value, output_attentions, use_cache, padding_mask, position_embeddings, *args, **kwargs)\u001b[39m\n\u001b[32m    527\u001b[39m             Q = Q.view(bsz, q_len, n_kv_heads, n_groups, head_dim)\n\u001b[32m    528\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m529\u001b[39m     A = \u001b[43mxformers_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    530\u001b[39m     A = A.view(bsz, q_len, n_heads, head_dim)\n\u001b[32m    532\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m HAS_FLASH_ATTENTION \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/xformers/ops/fmha/__init__.py:308\u001b[39m, in \u001b[36mmemory_efficient_attention\u001b[39m\u001b[34m(query, key, value, attn_bias, p, scale, op, output_dtype)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmemory_efficient_attention\u001b[39m(\n\u001b[32m    197\u001b[39m     query: torch.Tensor,\n\u001b[32m    198\u001b[39m     key: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    205\u001b[39m     output_dtype: Optional[torch.dtype] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    206\u001b[39m ) -> torch.Tensor:\n\u001b[32m    207\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Implements the memory-efficient attention mechanism following\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[33;03m    `\"Self-Attention Does Not Need O(n^2) Memory\" <http://arxiv.org/abs/2112.05682>`_.\u001b[39;00m\n\u001b[32m    209\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    306\u001b[39m \u001b[33;03m    :return: multi-head attention Tensor with shape ``[B, Mq, H, Kv]``\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_memory_efficient_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mInputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m            \u001b[49m\u001b[43mp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m            \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m        \u001b[49m\u001b[43mop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    319\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/xformers/ops/fmha/__init__.py:469\u001b[39m, in \u001b[36m_memory_efficient_attention\u001b[39m\u001b[34m(inp, op)\u001b[39m\n\u001b[32m    464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_memory_efficient_attention\u001b[39m(\n\u001b[32m    465\u001b[39m     inp: Inputs, op: Optional[AttentionOp] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    466\u001b[39m ) -> torch.Tensor:\n\u001b[32m    467\u001b[39m     \u001b[38;5;66;03m# fast-path that doesn't require computing the logsumexp for backward computation\u001b[39;00m\n\u001b[32m    468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(x.requires_grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m [inp.query, inp.key, inp.value]):\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_memory_efficient_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m            \u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mop\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    471\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    473\u001b[39m     output_shape = inp.normalize_bmhk()\n\u001b[32m    475\u001b[39m     op_fw = _serialize_op(op[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/xformers/ops/fmha/__init__.py:492\u001b[39m, in \u001b[36m_memory_efficient_attention_forward\u001b[39m\u001b[34m(inp, op)\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    490\u001b[39m     _ensure_op_supports_or_raise(\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mmemory_efficient_attention\u001b[39m\u001b[33m\"\u001b[39m, op, inp)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m out, *_ = \u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out.reshape(output_shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/xformers/ops/fmha/cutlass.py:250\u001b[39m, in \u001b[36mFwOp.apply\u001b[39m\u001b[34m(cls, inp, needs_gradient)\u001b[39m\n\u001b[32m    245\u001b[39m         value = inp.value[:, :, group]\n\u001b[32m    246\u001b[39m         bias = _attn_bias_apply(\n\u001b[32m    247\u001b[39m             inp.attn_bias, partial(torch.select, dim=\u001b[32m1\u001b[39m, index=group)\n\u001b[32m    248\u001b[39m         )\n\u001b[32m    249\u001b[39m         outs.append(\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m             \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_bmhk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m                \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m                \u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m         )\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m streams[\u001b[32m1\u001b[39m:]:\n\u001b[32m    256\u001b[39m     main_stream.wait_stream(s)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/xformers/ops/fmha/cutlass.py:273\u001b[39m, in \u001b[36mFwOp.apply_bmhk\u001b[39m\u001b[34m(cls, inp, needs_gradient)\u001b[39m\n\u001b[32m    271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsupported attn_bias type\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    272\u001b[39m seqstart_k, seqstart_q, max_seqlen_q, max_seqlen_k = _get_seqlen_info(inp)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m out, lse, rng_seed, rng_offset, _, _ = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mOPERATOR\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m=\u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m=\u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_get_tensor_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_q\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseqstart_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseqstart_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seqlen_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_seqlen_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompute_log_sumexp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mneeds_gradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_mask_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_custom_mask_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseqlen_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m.\u001b[49m\u001b[43mk_seqinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mseqlen\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m            \u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBlockDiagonalCausalWithOffsetPaddedKeysMask\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    292\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[43m        \u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_window_size\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    296\u001b[39m \u001b[43m            \u001b[49m\u001b[43minp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mattn_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    297\u001b[39m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m                \u001b[49m\u001b[43mBlockDiagonalCausalLocalAttentionMask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[43m                \u001b[49m\u001b[43mBlockDiagonalCausalLocalAttentionFromBottomRightMask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m                \u001b[49m\u001b[43mLowerTriangularFromBottomRightLocalAttentionMask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    306\u001b[39m ctx: Optional[Context] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    307\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m needs_gradient:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.13/site-packages/torch/_ops.py:1158\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1156\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1157\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1158\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA driver error: invalid argument"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()\n",
    "\n",
    "# æ‰“å°è®­ç»ƒç»Ÿè®¡ä¿¡æ¯\n",
    "print(trainer_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹ï¼ˆLoraï¼‰\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥å†æ¬¡è¿›è¡Œæ¨ç†ï¼Œæ¯”è¾ƒå¾®è°ƒåçš„æ¨¡å‹ä¸åŸå§‹æ¨¡å‹çš„å·®å¼‚ã€‚å¦‚æœå¯¹ç»“æœæ»¡æ„ï¼Œå¯ä»¥ä½¿ç”¨ `model.save_pretrained(\"your_lora_adapter_path\")` æ¥ä¿å­˜è®­ç»ƒå¥½çš„ LoRA é€‚é…å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"qwen-1.5b_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('qwen-1.5b_lora_model/tokenizer_config.json',\n",
       " 'qwen-1.5b_lora_model/special_tokens_map.json',\n",
       " 'qwen-1.5b_lora_model/chat_template.jinja',\n",
       " 'qwen-1.5b_lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"qwen-1.5b_lora_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡å‹ä¿å­˜æ–¹å¼äºŒé€‰ä¸€ï¼ˆè¦ä¹ˆä½¿ç”¨ä¸Šé¢çš„åˆ†å¼€ä¿å­˜ï¼Œè¦ä¹ˆä½¿ç”¨è¿™é‡Œçš„åˆå¹¶ Lora ä¿å­˜ï¼‰\n",
    "# model.save_pretrained_merged(\"qwen-1.5b_lora_model\", tokenizer, save_method=\"merged_16bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. æµ‹è¯•è®­ç»ƒåçš„ç”Ÿæˆç»“æœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "\n",
    "question=\"ä¸€ä¸ªæ‚£æœ‰æ€¥æ€§é˜‘å°¾ç‚çš„ç—…äººå·²ç»å‘ç—…5å¤©ï¼Œè…¹ç—›ç¨æœ‰å‡è½»ä½†ä»ç„¶å‘çƒ­ï¼Œåœ¨ä½“æ£€æ—¶å‘ç°å³ä¸‹è…¹æœ‰å‹ç—›çš„åŒ…å—ï¼Œæ­¤æ—¶åº”å¦‚ä½•å¤„ç†ï¼Ÿ\", # Question\n",
    "inputs = tokenizer([inference_prompt.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1000,\n",
    "    use_cache=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "è¿™ä¸ªç—…äººå·²ç»5å¤©äº†ï¼Œæ€¥æ€§é˜‘å°¾ç‚ï¼Œæ„Ÿè§‰è…¹ç—›åªæ˜¯ç¨å¾®å‡è½»äº†ä¸€ç‚¹ï¼Œä½†æ˜¯è¿˜æ˜¯æœ‰ç‚¹å‘çƒ­ã€‚æ‰€ä»¥ï¼Œä»–çš„ç–¼ç—›å’Œå‘çƒ­æƒ…å†µçœ‹èµ·æ¥è¿˜æ˜¯æŒºæ­£å¸¸çš„ã€‚\n",
      "\n",
      "æ ¹æ®ä»–çš„ç—‡çŠ¶ï¼Œæˆ‘è§‰å¾—ä»–åº”è¯¥è¿˜æ˜¯æœ‰åŠæ³•å»å¤„ç†çš„ã€‚å°±æ˜¯è¯´ï¼Œè™½ç„¶ç–¼ç—›å’Œå‘çƒ­çœ‹èµ·æ¥æœ‰ç‚¹è½»ï¼Œä½†ä¸è¦è¿‡äºæ‹…å¿ƒï¼Œæ¯•ç«Ÿé˜‘å°¾ç‚ç¡®å®ä¸æ˜¯é‚£ä¹ˆå®¹æ˜“æ§åˆ¶çš„ã€‚\n",
      "\n",
      "å—¯ï¼Œæƒ³åˆ°è¿™é‡Œï¼Œæˆ‘æƒ³åˆ°è¦æ‰¾ä¸€ä¸ªåˆé€‚çš„æ²»ç–—æ–¹æ³•ï¼Œå°±æ˜¯æŠŠç–¼ç—›å’Œå‘çƒ­éƒ½æ§åˆ¶ä½ï¼Œç„¶åçœ‹çœ‹ä»–çš„ç–¼ç—›å’Œå‘çƒ­æƒ…å†µæ˜¯å¦è¿˜æœ‰å…¶ä»–å˜åŒ–ã€‚\n",
      "\n",
      "å¯èƒ½ï¼Œè¿™ä¸ªç—…äººéœ€è¦è¿›è¡Œä¸€äº›æ’ç—›å’ŒæŠ—ç‚æ²»ç–—ï¼Œä½†å‰ææ˜¯ä»–çš„ç–¼ç—›å’Œå‘çƒ­ç—‡çŠ¶æ²¡æœ‰å¤ªå¤§çš„æ”¹å–„ã€‚å¦‚æœç—‡çŠ¶æ²¡æœ‰æ”¹å–„ï¼Œé‚£ä¹ˆå¯èƒ½éœ€è¦è€ƒè™‘æ›´å¤æ‚çš„æ²»ç–—æ–¹æ³•ï¼Œæ¯”å¦‚æ‰‹æœ¯ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œä»ä»–çš„ç—‡çŠ¶æ¥çœ‹ï¼Œç–¼ç—›å’Œå‘çƒ­å·²ç»æœ‰æ‰€å‡è½»ï¼Œè€Œä¸”åœ¨ä½“æ£€æ—¶è¿˜å‘ç°äº†å³ä¸‹è…¹çš„å‹ç—›åŒ…å—ï¼Œè¿™è®©æˆ‘æƒ³åˆ°ï¼Œå¯èƒ½å¯ä»¥è€ƒè™‘ä½¿ç”¨ä¸€äº›è¯ç‰©æ¥ç¼“è§£ç–¼ç—›å’Œå‘çƒ­ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œå¦‚æœç–¼ç—›å’Œå‘çƒ­ç—‡çŠ¶æ²¡æœ‰æ”¹å–„ï¼Œæˆ–è€…ç—‡çŠ¶å˜å¾—è¶Šæ¥è¶Šé‡ï¼Œé‚£ä¹ˆå¯èƒ½éœ€è¦æ‰‹æœ¯æ¥å¤„ç†è¿™ç§æƒ…å†µã€‚\n",
      "\n",
      "æ‰€ä»¥ï¼Œç»¼åˆæ¥çœ‹ï¼Œè¿™ä¸ªç—…äººåœ¨5å¤©çš„æ²»ç–—æ—¶é—´é‡Œï¼Œåº”è¯¥æ ¹æ®ç—‡çŠ¶çš„å…·ä½“æƒ…å†µæ¥å†³å®šé‡‡å–ä»€ä¹ˆæ²»ç–—ã€‚\n",
      "</think>\n",
      "æ ¹æ®ç—…äºº5å¤©çš„æ²»ç–—æ—¶é—´ï¼Œç—‡çŠ¶æœ‰æ‰€å‡è½»ä½†å‘çƒ­å’Œç–¼ç—›å¹¶æœªæ”¹å–„ï¼Œå› æ­¤éœ€è¦è€ƒè™‘è¿›ä¸€æ­¥çš„æ²»ç–—ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå»ºè®®ä½¿ç”¨éä¾µå…¥æ€§ç–¼ç—›æ²»ç–—ï¼Œå¦‚æ­¢ç—›è¯ï¼Œä»¥ç¼“è§£ç–¼ç—›ç—‡çŠ¶ã€‚åŒæ—¶ï¼Œä½¿ç”¨æŠ—ç‚è¯ç‰©å¦‚é˜¿å¸åŒ¹æ—æˆ–é˜¿ç«‹å…‹å¸ä»–æ¥å‡è½»å‘çƒ­ã€‚å¦‚æœç—‡çŠ¶æ²¡æœ‰æ”¹å–„ï¼Œå»ºè®®è¿›è¡Œæ‰‹æœ¯ä»¥å¤„ç†å‹ç—›åŒ…å—ã€‚\n"
     ]
    }
   ],
   "source": [
    "output = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "print(output[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(question: str, model, tokenizer, inference_prompt: str, max_new_tokens: int = 1024) -> str:\n",
    "    \"\"\"\n",
    "    ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹å’Œåˆ†è¯å™¨ä¸ºç»™å®šçš„åŒ»å­¦é—®é¢˜ç”Ÿæˆå“åº”ã€‚\n",
    "\n",
    "    Args:\n",
    "        question (str): éœ€è¦æ¨¡å‹å›ç­”çš„åŒ»å­¦é—®é¢˜ã€‚\n",
    "        model: å·²åŠ è½½çš„ Unsloth/Hugging Face æ¨¡å‹ã€‚\n",
    "        tokenizer: å¯¹åº”çš„åˆ†è¯å™¨ã€‚\n",
    "        inference_prompt (str): ç”¨äºæ ¼å¼åŒ–è¾“å…¥çš„ f-string æ¨¡æ¿ã€‚\n",
    "        max_new_tokens (int, optional): ç”Ÿæˆå“åº”çš„æœ€å¤§ token æ•°é‡ã€‚é»˜è®¤ä¸º 1024ã€‚\n",
    "\n",
    "    Returns:\n",
    "        str: æ¨¡å‹ç”Ÿæˆçš„å“åº”æ–‡æœ¬ï¼Œå·²å»é™¤ prompt éƒ¨åˆ†ã€‚\n",
    "    \"\"\"\n",
    "    # 1. ä½¿ç”¨æ¨¡æ¿æ ¼å¼åŒ–è¾“å…¥\n",
    "    prompt = inference_prompt.format(\n",
    "        question, # å¡«å……é—®é¢˜\n",
    "        \"\",       # ç•™ç©ºï¼Œè®©æ¨¡å‹ç”Ÿæˆ CoT å’Œ Response\n",
    "    )\n",
    "\n",
    "    # 2. å°†æ ¼å¼åŒ–åçš„ prompt è¿›è¡Œåˆ†è¯ï¼Œå¹¶è½¬ç§»åˆ° GPU\n",
    "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    # 3. ä½¿ç”¨æ¨¡å‹ç”Ÿæˆè¾“å‡º\n",
    "    # use_cache=True ç”¨äºåŠ é€Ÿè§£ç è¿‡ç¨‹\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        attention_mask=inputs.attention_mask,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        use_cache=True,\n",
    "    )\n",
    "    \n",
    "    # 4. å°†ç”Ÿæˆçš„ token è§£ç ä¸ºæ–‡æœ¬\n",
    "    # skip_special_tokens=True ä¼šç§»é™¤åƒ EOS_TOKEN è¿™æ ·çš„ç‰¹æ®Šæ ‡è®°\n",
    "    decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "\n",
    "    # 5. åˆ‡åˆ†å­—ç¬¦ä¸²ï¼Œåªè¿”å› \"### Response:\" ä¹‹åçš„éƒ¨åˆ†\n",
    "    # ä½¿ç”¨ .split() åˆ†å‰²å¹¶è·å–å“åº”å†…å®¹ï¼Œ.strip() ç”¨äºå»é™¤å¯èƒ½å­˜åœ¨çš„å‰åç©ºç™½å­—ç¬¦\n",
    "    response_part = decoded_output.split(\"### Response:\")\n",
    "    if len(response_part) > 1:\n",
    "        return response_part[1].strip()\n",
    "    else:\n",
    "        # å¦‚æœæ¨¡å‹æ²¡æœ‰ç”Ÿæˆ \"### Response:\" æ ‡è®°ï¼Œåˆ™è¿”å›æ•´ä¸ªç”Ÿæˆå†…å®¹ä»¥ä¾›è°ƒè¯•\n",
    "        return decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "å—¯ï¼Œè¿™ä½60å²çš„ç”·æ€§æ‚£è€…æœ‰å³ä¾§èƒ¸ç–¼ï¼ŒXçº¿æ£€æŸ¥æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¿™è®©æˆ‘æƒ³åˆ°å¯èƒ½æœ‰è‚ºç»“æ ¸çš„è¿¹è±¡ã€‚è‚ºç»“æ ¸é€šå¸¸ä¼šå¯¼è‡´èƒ¸è…”ç§¯æ¶²ï¼Œç‰¹åˆ«æ˜¯å³ä¾§çš„ã€‚è¦å¼„æ¸…æ¥šè¿™ç§ç§¯æ¶²çš„å…·ä½“æ€§è´¨ï¼Œå®éªŒå®¤æ£€æŸ¥åº”è¯¥æ˜¯å…³é”®ã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæ£€æŸ¥èƒ¸è…”æ¶²ä½“çš„é…¸ç¢±åº¦ï¼Œèƒ½å‘Šè¯‰æˆ‘è¿™ä¸ªç§¯æ¶²æ˜¯é…¸æ€§è¿˜æ˜¯ç¢±æ€§ã€‚é…¸æ€§æˆ–ç¢±æ€§å¯ä»¥å¸®åŠ©æˆ‘ä»¬ç¡®å®šæ˜¯ä»€ä¹ˆç±»å‹çš„æ¶²ä½“ï¼Œæ¯”å¦‚ä¹³é…¸æ°´æˆ–è€…ä¹³é…¸é’ ã€‚å¦‚æœæ€€ç–‘æ˜¯ä¹³é…¸æ°´ï¼Œå¯ä»¥ç”¨ä¹³é…¸æ°´é…¶çš„æ£€æµ‹æ–¹æ³•æ¥è¿›ä¸€æ­¥ç¡®è®¤ã€‚\n",
      "\n",
      "ç„¶åï¼Œçœ‹çœ‹æ˜¯å¦æœ‰ä¹³é…¸é’ çš„å­˜åœ¨ï¼Œè¿™åœ¨è‚ºç»“æ ¸æ‚£è€…ä¸­éå¸¸é‡è¦ï¼Œå› ä¸ºä¹³é…¸é’ å¯ä»¥é‡Šæ”¾ä¹³é…¸ï¼Œå¸®åŠ©ç»´æŒé…¸æ€§ç¯å¢ƒã€‚å¦‚æœæ£€æµ‹åˆ°ä¹³é…¸é’ ï¼Œå°±èƒ½è¿›ä¸€æ­¥éªŒè¯ç§¯æ¶²çš„ç±»å‹ã€‚\n",
      "\n",
      "æ­¤å¤–ï¼Œæ£€æµ‹å°¿æ¶²ä¸­çš„ä¹³é…¸å«é‡ä¹Ÿå¾ˆé‡è¦ï¼Œå› ä¸ºè¿™èƒ½ç›´æ¥åæ˜ è‚ºç»“æ ¸æ‚£è€…çš„ä¹³é…¸å«é‡ï¼Œä»è€Œåˆ¤æ–­æ˜¯å¦å·²ç»äº§ç”Ÿäº†ä¹³é…¸æ°´ã€‚\n",
      "\n",
      "æœ€åï¼Œç”¨è¡€æ¸…ä¸­çš„è¡€é’™å«é‡æ¥åˆ¤æ–­è¡€é’™æ°´å¹³ï¼Œå› ä¸ºè¡€é’™å‡é«˜å¯èƒ½æç¤ºè‚ºç»“æ ¸æ‚£è€…çš„è¡€é’™æ°´å¹³è¾ƒé«˜ï¼Œä¹Ÿå¯èƒ½æš—ç¤ºäº†ä¹³é…¸å«é‡å¢åŠ ã€‚\n",
      "\n",
      "ç»¼ä¸Šæ‰€è¿°ï¼Œé€šè¿‡è¿™äº›æ£€æŸ¥ï¼Œæˆ‘ä»¬èƒ½æ›´å‡†ç¡®åœ°äº†è§£èƒ¸è…”ç§¯æ¶²çš„å…·ä½“æ€§è´¨å’Œç±»å‹ï¼Œä»è€Œåšå‡ºæ›´å‡†ç¡®çš„è¯Šæ–­ã€‚\n",
      "</think>\n",
      "```\n",
      "åœ¨è¯Šæ–­è‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²æ—¶ï¼Œäº†è§£èƒ¸è…”æ¶²ä½“çš„é…¸ç¢±åº¦å’Œä¹³é…¸å«é‡æ˜¯å…³é”®ã€‚ä»¥ä¸‹æ˜¯è¿™äº›æ£€æŸ¥çš„æ­¥éª¤ï¼š\n",
      "\n",
      "1. **æ£€æŸ¥èƒ¸è…”æ¶²ä½“çš„é…¸ç¢±åº¦**ï¼šé€šè¿‡é…¸ç¢±åº¦è®¡æ£€æµ‹ï¼Œå¯ä»¥ç¡®å®šèƒ¸è…”æ¶²ä½“æ˜¯é…¸æ€§è¿˜æ˜¯ç¢±æ€§ã€‚é…¸æ€§æˆ–ç¢±æ€§å¯ä»¥å¸®åŠ©æˆ‘ä»¬åˆæ­¥åˆ¤æ–­æ˜¯ä¹³é…¸æ°´æˆ–ä¹³é…¸é’ ã€‚\n",
      "\n",
      "2. **æ£€æµ‹ä¹³é…¸é’ çš„å­˜åœ¨**ï¼šå¦‚æœæ€€ç–‘æ˜¯ä¹³é…¸æ°´ï¼Œå¯ä»¥ç”¨ä¹³é…¸æ°´é…¶æ¥æ£€æµ‹ï¼Œä»¥ç¡®è®¤ä¹³é…¸é’ çš„å­˜åœ¨ã€‚\n",
      "\n",
      "3. **å°¿æ¶²ä¸­çš„ä¹³é…¸æ£€æµ‹**ï¼šå°¿æ¶²ä¸­çš„ä¹³é…¸å«é‡å¯ä»¥åæ˜ è‚ºç»“æ ¸æ‚£è€…çš„ä¹³é…¸å«é‡ï¼Œä»è€Œåˆ¤æ–­æ˜¯å¦å·²å¼€å§‹äº§ç”Ÿä¹³é…¸æ°´ã€‚\n",
      "\n",
      "4. **è¡€æ¸…ä¸­çš„è¡€é’™æ£€æµ‹**ï¼šè¡€é’™æ°´å¹³çš„å˜åŒ–å¯ä»¥æç¤ºè¡€é’™çš„æ°´å¹³ï¼Œä»è€Œåˆ¤æ–­æ˜¯å¦ä¸ºè‚ºç»“æ ¸æ‚£è€…çš„è¡€é’™æ°´å¹³è¾ƒé«˜ã€‚\n",
      "\n",
      "é€šè¿‡è¿™äº›æ£€æŸ¥ï¼Œæˆ‘ä»¬èƒ½å¤Ÿæ›´å‡†ç¡®åœ°äº†è§£èƒ¸è…”ç§¯æ¶²çš„å…·ä½“æ€§è´¨å’Œç±»å‹ï¼Œä»è€Œåšå‡ºæ›´å‡†ç¡®çš„è¯Šæ–­ã€‚\n"
     ]
    }
   ],
   "source": [
    "my_question = \"å¯¹äºä¸€å60å²ç”·æ€§æ‚£è€…ï¼Œå‡ºç°å³ä¾§èƒ¸ç–¼å¹¶åœ¨Xçº¿æ£€æŸ¥ä¸­æ˜¾ç¤ºå³ä¾§è‚‹è†ˆè§’æ¶ˆå¤±ï¼Œè¯Šæ–­ä¸ºè‚ºç»“æ ¸ä¼´å³ä¾§èƒ¸è…”ç§¯æ¶²ï¼Œè¯·é—®å“ªä¸€é¡¹å®éªŒå®¤æ£€æŸ¥å¯¹äº†è§£èƒ¸æ°´çš„æ€§è´¨æ›´æœ‰å¸®åŠ©ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model, tokenizer, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== æ¨¡å‹å›ç­” ====================\n",
      "<think>\n",
      "è¿™ä½28å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œé•¿æœŸç†¬å¤œï¼Œæœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œç”šè‡³æœ‰ç‚¹æ¶å¿ƒï¼Œè¿™è®©æˆ‘æƒ³åˆ°ä»–å¯èƒ½å¾—äº†è„‘æŸä¼¤æ€§æ™•çœ©ç—‡ã€‚\n",
      "\n",
      "è„‘æŸä¼¤æ€§æ™•çœ©ç—‡æ˜¯ä¸€ç§å¸¸è§çš„ç¥ç»ç³»ç»Ÿç–¾ç—…ï¼Œå°¤å…¶æ˜¯è„‘æŸä¼¤åï¼Œå¯¼è‡´è„‘ç¼ºæ°§å’Œç¼ºè¡€ï¼Œä»è€Œå¼•èµ·æ™•çœ©ç­‰ç—‡çŠ¶ã€‚è¿™ç§ç–¾ç—…é€šå¸¸è¡¨ç°ä¸ºè„‘ç¼ºæ°§ï¼Œè¿›è€Œå¯¼è‡´æ™•çœ©ï¼Œå°¤å…¶åœ¨é•¿æ—¶é—´çš„è„‘æŸä¼¤åï¼Œæ™•çœ©ç—‡çŠ¶ä¼šæ›´åŠ æ˜æ˜¾ã€‚\n",
      "\n",
      "å†ä»”ç»†æƒ³æƒ³ï¼Œè¿™ä½æ‚£è€…é•¿æ—¶é—´ç†¬å¤œï¼Œè¿™å¯èƒ½æ˜¯å› ä¸ºè„‘ç¼ºæ°§å¯¼è‡´çš„ç–²åŠ³ï¼Œç”šè‡³æ›´å¯èƒ½å¯¼è‡´è„‘æŸä¼¤æ€§æ™•çœ©ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ™•çœ©ä¸ä»…ä»…æ˜¯ç®€å•çš„ç–²åŠ³ï¼Œè€Œæ˜¯ä¸è„‘ç¼ºæ°§å’Œç¼ºè¡€ç›´æ¥ç›¸å…³ã€‚\n",
      "\n",
      "è„‘æŸä¼¤æ€§æ™•çœ©ç—‡çš„ç‰¹å¾åŒ…æ‹¬è„‘ç¼ºæ°§ã€ç¼ºè¡€ã€ç¼ºæ°§è¡€æµé‡å’Œè„‘ç¼ºæ°§è¡€æµé‡ï¼Œè¿™äº›éƒ½æ˜¯å¯¼è‡´æ™•çœ©çš„é‡è¦åŸå› ã€‚è¿™ç§ç–¾ç—…ä¹Ÿå¸¸ä¼´éšè„‘æ°´è‚¿å’Œè„‘æŸä¼¤ï¼Œæ‰€ä»¥æ‚£è€…çš„æƒ…å†µå¯èƒ½å’Œè„‘æ°´è‚¿æœ‰å…³ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œè„‘æŸä¼¤æ€§æ™•çœ©ç—‡çš„ç—…å› å…¶å®ä¸æ˜¯å•ä¸€çš„ï¼Œå®ƒä¸ä»…å—è„‘æŸä¼¤çš„å½±å“ï¼Œè¿˜å¯èƒ½å—åˆ°è„‘æŸä¼¤å‰è„‘æ°´è‚¿çš„å½±å“ã€‚è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚£è€…å¯èƒ½éœ€è¦è€ƒè™‘è„‘æ°´è‚¿ä½œä¸ºå¯èƒ½çš„ç—…å› ã€‚\n",
      "\n",
      "æ€»çš„æ¥è¯´ï¼Œè¿™ä½æ‚£è€…å¯èƒ½å¾—äº†è„‘æŸä¼¤æ€§æ™•çœ©ç—‡ï¼Œå› ä¸ºä»–çš„ç—‡çŠ¶å’Œç—…å› éƒ½ç¬¦åˆè„‘æŸä¼¤åå‡ºç°çš„å…¸å‹è¡¨ç°ã€‚ä¸è¿‡ï¼Œç”±äºç—…å› å¤æ‚ï¼Œå»ºè®®ä»–å°½å¿«å»çœ‹è„‘æ°´è‚¿ä¸“å®¶ï¼Œä»¥æ’é™¤è„‘æ°´è‚¿çš„å¯èƒ½ï¼Œä»è€Œæ›´å¥½åœ°è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
      "</think>\n",
      "```\n",
      "æ ¹æ®æ‚£è€…çš„ç—‡çŠ¶å’Œç—…å› åˆ†æï¼Œè¿™ä½28å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œé•¿æœŸç†¬å¤œï¼Œæœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œç”šè‡³æœ‰ç‚¹æ¶å¿ƒï¼Œè¿™æç¤ºä»–å¯èƒ½å¾—äº†è„‘æŸä¼¤æ€§æ™•çœ©ç—‡ã€‚è„‘æŸä¼¤æ€§æ™•çœ©ç—‡æ˜¯ä¸€ç§å¸¸è§çš„ç¥ç»ç³»ç»Ÿç–¾ç—…ï¼Œå°¤å…¶æ˜¯è„‘æŸä¼¤åï¼Œå¯¼è‡´è„‘ç¼ºæ°§å’Œç¼ºè¡€ï¼Œä»è€Œå¼•èµ·æ™•çœ©ã€‚è¿™ç§ç–¾ç—…é€šå¸¸è¡¨ç°ä¸ºè„‘ç¼ºæ°§ï¼Œè¿›è€Œå¯¼è‡´æ™•çœ©ï¼Œå°¤å…¶åœ¨é•¿æ—¶é—´çš„è„‘æŸä¼¤åï¼Œæ™•çœ©ç—‡çŠ¶ä¼šæ›´åŠ æ˜æ˜¾ã€‚\n",
      "\n",
      "æ­¤å¤–ï¼Œè„‘æŸä¼¤æ€§æ™•çœ©ç—‡çš„ç—…å› ä¹Ÿå—è„‘æŸä¼¤å‰è„‘æ°´è‚¿çš„å½±å“ï¼Œè¿™ç§æƒ…å†µä¸‹ï¼Œæ‚£è€…å¯èƒ½éœ€è¦è€ƒè™‘è„‘æ°´è‚¿ä½œä¸ºå¯èƒ½çš„ç—…å› ã€‚å› æ­¤ï¼Œå»ºè®®å°½å¿«æŸ¥çœ‹è„‘æ°´è‚¿ä¸“å®¶ï¼Œä»¥æ’é™¤è„‘æ°´è‚¿çš„å¯èƒ½ï¼Œä»è€Œæ›´å¥½åœ°è¯Šæ–­å’Œæ²»ç–—ã€‚\n",
      "\n",
      "ç»¼ä¸Šæ‰€è¿°ï¼Œè¿™ä½æ‚£è€…å¯èƒ½å¾—äº†è„‘æŸä¼¤æ€§æ™•çœ©ç—‡ï¼Œä½†å»ºè®®å°½å¿«å¯»æ±‚ä¸“ä¸šè¯Šæ–­ï¼Œä»¥ç¡®ä¿å…¶ç—…æƒ…å¾—åˆ°åŠæ—¶å‡†ç¡®çš„æ²»ç–—ã€‚\n"
     ]
    }
   ],
   "source": [
    "my_question = \"å¯¹äºä¸€å 28 å²çš„ç”·æ€§æ‚£è€…ï¼Œå·¥ä½œæ˜¯ç¨‹åºå‘˜ï¼Œå¸¸å¹´ç†¬å¤œï¼Œæœ€è¿‘çªç„¶æ„Ÿè§‰å¤´æ™•ç›®çœ©ï¼Œç”šè‡³æœ‰ç‚¹æ¶å¿ƒã€‚è¯·é—®æœ‰å¯èƒ½æ˜¯ä»€ä¹ˆç–¾ç—…ï¼Ÿ\"\n",
    "\n",
    "response = generate_response(my_question, model, tokenizer, inference_prompt)\n",
    "print(\"==================== æ¨¡å‹å›ç­” ====================\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
