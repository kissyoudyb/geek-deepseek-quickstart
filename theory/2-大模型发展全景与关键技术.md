## 大模型发展全景与关键技术 -- 课后作业

### 单项选择题 (共15题)

1.  Transformer架构的提出，对自然语言处理领域产生了深远影响，其核心机制是什么？
    A. 循环神经网络 (RNN)
    B. 卷积神经网络 (CNN)
    C. 自注意力机制 (Self-Attention)
    D. 梯度提升决策树 (GBDT)

2.  在Transformer模型中，引入位置编码（Positional Encoding）的主要目的是什么？
    A. 增加模型的非线性表达能力
    B. 解决梯度消失问题
    C. 为模型提供序列中词语的顺序信息
    D. 减少模型的参数数量

3.  多头注意力（Multi-Head Attention）机制相比于单头注意力，其主要优势在于？
    A. 显著降低了计算复杂度
    B. 能够让模型从不同角度、不同子空间学习信息并进行并行计算
    C. 简化了模型的结构
    D. 增强了模型处理长序列的能力

4.  以下哪项是大语言模型（LLM）区别于传统机器学习模型最显著的特点之一？
    A. 仅能处理文本数据
    B. 训练数据规模相对较小
    C. 表现出“涌现能力”，如上下文学习和思维链推理
    D. 必须依赖人工特征工程

5.  “预训练+微调”（Pre-training + Fine-tuning）是大模型时代常见的训练范式，其中“预训练”阶段的主要目标是？
    A. 针对特定下游任务优化模型性能
    B. 让模型从海量无标注或弱标注数据中学习通用的语言知识和模式
    C. 降低模型在特定任务上的过拟合风险
    D. 快速验证模型架构的有效性

6.  在处理超长序列数据时，标准自注意力机制面临的主要挑战是什么？
    A. 难以捕捉局部依赖关系
    B. 计算复杂度和内存消耗随序列长度呈平方级增长
    C. 无法进行并行计算
    D. 容易产生梯度爆炸

7.  稀疏注意力（Sparse Attention）机制，如Longformer和BigBird中的设计，主要是为了解决什么问题？
    A. 提高模型对不同语言的翻译能力
    B. 增强模型的可解释性
    C. 降低处理长序列时的计算复杂度和内存占用
    D. 提升模型生成文本的创造性

8.  混合专家模型（MoE, Mixture of Experts）的核心思想是什么？
    A. 将多个不同架构的模型集成在一起
    B. 通过门控网络（Gating Network）为每个输入动态选择一部分“专家”子网络进行处理，以扩大模型容量同时控制计算量
    C. 让所有专家网络并行处理相同输入，然后对结果进行投票
    D. 每个专家网络都针对一个完全不同的任务进行训练

9.  模型量化（Quantization）是一种常见的模型压缩技术，其基本原理是？
    A. 移除模型中不重要的权重或连接
    B. 将模型从一个大模型（教师模型）的知识迁移到小模型（学生模型）
    C. 降低模型参数（权重和/或激活值）的数值表示精度，如从FP32转换为INT8
    D. 搜索更小、更高效的网络结构

10. 知识蒸馏（Knowledge Distillation）的目标是？
    A. 提高模型训练数据的质量
    B. 将一个大型、复杂的“教师模型”的知识迁移到一个更小、更高效的“学生模型”
    C. 直接减少教师模型的参数量
    D. 为模型参数添加噪声以增强鲁棒性

11. 根据AI发展历史，Transformer架构的诞生通常被认为是哪个阶段的关键成果？
    A. 弱人工智能阶段
    B. 统计机器学习阶段
    C. 深度学习阶段（并开启了大语言模型阶段）
    D. 符号主义AI阶段

12. NNLM (Neural Network Language Model)相比于传统的N-Gram模型，其主要创新在于？
    A. 引入了专家知识规则
    B. 使用概率分布统计词频
    C. 将语言模型问题从统计学方法转化为基于神经网络的目标函数优化问题，并引入了词向量的概念
    D. 大幅提升了模型的并行计算能力

13. RoPE (Rotary Positional Encoding) 作为一种相对位置编码方法，相比于Transformer原始论文中的固定正余弦位置编码，其关键优势在于？
    A. 实现简单，计算量极小
    B. 更好地支持模型对超长序列的外推能力和长文本的位置感知
    C. 无需任何额外参数
    D. 只能编码绝对位置信息

14. 大模型的“涌现能力”（Emergent Abilities）通常在什么情况下开始显现？
    A. 模型参数规模较小，训练数据量少时
    B. 模型采用特定激活函数时
    C. 模型参数规模和训练数据量跨越某个“临界点”之后
    D. 模型仅在特定任务上微调后

15. DeepSeekMoE架构中提到的“细粒度专家分割”和“设备限制路由”等设计，主要目标是？
    A. 提升模型的多语言处理能力
    B. 增强模型的视觉理解能力
    C. 进一步优化MoE模型的训练和推理效率，提升专家专业化能力并降低通信开销
    D. 简化模型的部署流程

### 多项选择题 (共5题)

16. 以下哪些是大模型相比于传统机器学习模型的主要差异点？
    A. 参数规模和训练数据量远超传统模型
    B. 训练范式普遍采用“预训练+微调/指令跟随”
    C. 往往展现出更强的泛化能力和“涌现能力”
    D. 开发和使用上更依赖“提示工程”（Prompt Engineering）
    E. 对算力的要求通常较低

17. Transformer架构的关键组成部分包括哪些？
    A. 自注意力机制 (Self-Attention)
    B. 多头注意力机制 (Multi-Head Attention)
    C. 位置编码 (Positional Encoding)
    D. 前馈神经网络 (Feed Forward Network)
    E. 循环连接 (Recurrent Connections)

18. 混合专家模型（MoE）能够带来的潜在好处包括？
    A. 在保持或提升性能的同时，显著增加模型总参数量
    B. 在推理时，每个输入token只需要激活一部分专家，从而降低实际计算量
    C. 允许模型学习到更专门化的知识（每个专家可能关注不同方面）
    D. 训练成本和复杂度一定低于同等参数规模的稠密模型
    E. 天然适合并行化训练和推理

19. 模型压缩与加速技术主要为了解决大模型的哪些挑战？
    A. 模型体积过大，难以在资源受限设备上部署
    B. 推理延迟高，影响用户体验
    C. 训练成本过高
    D. 训练数据量不足
    E. 推理能耗大，运行成本高

20. 以下哪些技术属于模型压缩与加速的范畴？
    A. 模型量化 (Quantization)
    B. 知识蒸馏 (Knowledge Distillation)
    C. 模型剪枝 (Pruning)
    D. 数据增强 (Data Augmentation)
    E. 学习率调度 (Learning Rate Scheduling)
